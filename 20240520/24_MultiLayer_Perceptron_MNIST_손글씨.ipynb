{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412d7b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%config Completer.use_jedi = False\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c325f20",
   "metadata": {},
   "source": [
    "<img src=\"./MNIST.png\" align=\"left\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad494b3",
   "metadata": {},
   "source": [
    "MNIST 손글씨 실습을 위해서 케라스에서 제공하는 MNIST 데이터 셋을 사용한다.\n",
    "\n",
    "학습 데이터에는 60,000개의 샘플 데이터가 있고 테스트 데이터에는 총 10,000개의 샘플 데이터가 있다.  \n",
    "MNIST 손글씨 데이터는 이미지 하나가 28개의 행과 28개의 열을 가지는 픽셀 데이터이고 각 픽셀은 흑백 사진과 같이 0부터 255까지의 그레이스케일을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30d5f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28), y_train.shape: (60000,)\n",
      "x_test.shape: (10000, 28, 28), y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print('x_train.shape: {}, y_train.shape: {}'.format(x_train.shape, y_train.shape))\n",
    "print('x_test.shape: {}, y_test.shape: {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01227c9c",
   "metadata": {},
   "source": [
    "6만개의 학습 데이터를 학습 데이터(5만개)와 검증 데이터(1만개)로 분리한다.  \n",
    "학습 중간마다 검증 데이터로 모델 성능을 측정하면 모델 학습이 제대로 진행되는지 검증 정확도를 알 수 있고 학습 정확도는 올라가는데 검증 정확도가 더 이상 올라가지 않거나 오히려 떨어질 경우 학습의 조기 종료를 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc112c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (50000, 28, 28), y_train.shape: (50000,)\n",
      "x_val.shape: (10000, 28, 28), y_val.shape: (10000,)\n",
      "x_test.shape: (10000, 28, 28), y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터로 사용하기 위해서 학습 데이터에서 1만개를 분리한다.\n",
    "x_val = x_train[50000:] # 28행 28열로 구성된 검증 데이터\n",
    "x_train = x_train[:50000] # 28행 28열로 구성된 학습 데이터\n",
    "y_val = y_train[50000:] # 검증 데이터 레이블\n",
    "y_train = y_train[:50000] # 학습 데이터 레이블\n",
    "print('x_train.shape: {}, y_train.shape: {}'.format(x_train.shape, y_train.shape)) # 학습\n",
    "print('x_val.shape: {}, y_val.shape: {}'.format(x_val.shape, y_val.shape)) # 학습 중 검증\n",
    "print('x_test.shape: {}, y_test.shape: {}'.format(x_test.shape, y_test.shape)) # 학습 후 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f94c71",
   "metadata": {},
   "source": [
    "학습 데이터를 출력해보면 데이터가 0부터 255사이의 숫자(그레이스케일)로 구성된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb5b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0 \n",
      "  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])\n",
    "for i in x_train[0]:\n",
    "    for j in i:\n",
    "        print('{:3d} '.format(j), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385067e5",
   "metadata": {},
   "source": [
    "다층 퍼셉트론을 사용한 손글씨 이미지 분류 작업 흐름도\n",
    "\n",
    "<img src=\"./MNIST2.png\" align=\"left\" width=\"1300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660dd5d",
   "metadata": {},
   "source": [
    "다층 퍼셉트론의 입력값은 무조건 1차원 형태의 배열만 가능하다.  \n",
    "MNIST 손글시 데이터를 다층 퍼셉트론의 입력값으로 사용할 수 있도록 넘파이의 reshape() 메소드를 사용해서 2차원 배열 형태의 데이터를 1차원 배열 형태로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05d8f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (50000, 28, 28)\n",
      "x_train.shape: (50000, 784)\n",
      "x_val.shape: (10000, 784)\n",
      "x_test.shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape: {}'.format(x_train.shape))\n",
    "# 28행 28열로 구성된 학습 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "# x_train = np.reshape(x_train, [50000, 784])\n",
    "x_train = x_train.reshape(50000, 784)\n",
    "# x_train = x_train.reshape(-1, 784)\n",
    "print('x_train.shape: {}'.format(x_train.shape))\n",
    "# 28행 28열로 구성된 검증 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "x_val = x_val.reshape(10000, 784)\n",
    "print('x_val.shape: {}'.format(x_val.shape))\n",
    "# 28행 28열로 구성된 테스트 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "print('x_test.shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7d08b",
   "metadata": {},
   "source": [
    "1차원으로 변경된 데이터를 그대로 다층 퍼셉트론의 입력되도 되지만 조금 더 효율적인 학습을 위해서 데이터를 정규화 한다.  \n",
    "데이터를 정규화 하면 모델의 학습 시간을 단축시키고 더 나은 성능을 보이는 효과가 있다.  \n",
    "MNIST 손글씨 데이터의 모든 값들은 0부터 255사이의 범위 안에 있으므로 255로 나눠 모든 값들이 0부터 1사이의 값으로 정규화 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa27570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 손글씨 데이터의 각 픽셀의 데이터 타입은 부호 없이 0부터 255사이의 값만 기억하면 되므로 데이터 타입이\n",
    "# 부호없는 8비트 정수(uint8)로 되어있다.\n",
    "# print(type(x_train[0][0])) # <class 'numpy.uint8'>\n",
    "# 255로 나눠서 0부터 1사이의 실수로 만들어야 하므로 astype() 메소드로 실수로 변환한다.\n",
    "x_train = x_train.astype(np.float32)\n",
    "# print(type(x_train[0][0])) # <class 'numpy.float32'>\n",
    "x_train /= 255\n",
    "x_val = x_val.astype(np.float32)\n",
    "x_val /= 255\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f25b5b",
   "metadata": {},
   "source": [
    "MNIST 손글씨 데이터 분류 모델은 0에서 9사이의 숫자로 분류하는 모델이므로 손실 함수로 크로스 엔트로피를 사용한다.  \n",
    "크로스 엔트로피를 계산하기 위해서 레이블(y_train, y_val, y_test)을 원-핫 인코딩(one-hot encoding)으로 변환한다.  \n",
    "원-한 인코딩은 데이터를 수많은 0과 1개의 1로 구별하는 인코딩 방식으로 0으로 이루어진 벡터 집합에서 단 1개의 1의 값으로 해당 데이터를 구별하는 것을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39549abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26fffdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# to_categorical() 메소드로 데이터에 원-핫 인코딩을 적용할 수 있다.\n",
    "print(y_train[:5])\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10) # 학습 데이터의 레이블에 원-핫 인코딩을 적용한다.\n",
    "for i in y_train[:5]:\n",
    "    print(i)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, 10) # 검증 데이터의 레이블에 원-핫 인코딩을 적용한다.\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10) # 테스트 데이터의 레이블에 원-핫 인코딩을 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7542131",
   "metadata": {},
   "source": [
    "입력 데이터는 784개의 숫자가 들어있는 1차원 배열 형태이다.  \n",
    "784개의 입력을 받는 256개의 노드가 첫 번째 히든 레이어에 있고 첫 번째 히든 레이어의 출력값을 입력으로 받는 두 번째 히든 레이어에는 128개의 노드가 있다.  \n",
    "두 번째 히든 레이어에는 과대 적합을 방지하기 위해 10%의 드롭 아웃을 적용한다.  \n",
    "세 번째 히든 레이어에서는 총 10개의 노드가 존재하며 이 10개의 노드값은 소프트 맥스를 통해서 0부터 0사이에 해당되는 각 숫자의 확률을 의미한다.  \n",
    "소프트 맥스는 분류해야하는 레이블의 총 개수를 k라고 할 때 k차원의 벡터를 입력받아 각 정답에 대한 확률을 추정한다.  \n",
    "소프트 맥스의 출력값과 실제값의 차이(오차)를 계산하기 위해 크로스 엔트로피를 손실 함수로 사용하고 손실 함수를 최소화 하기 위해서 Adam 옵티마이저(최적화 함수)를 사용해서 역전파를 통해 모든 가중치 및 편향값을 최적화 한다.\n",
    "\n",
    "최적화 함수 참고 사이트  \n",
    "https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam\n",
    "\n",
    "<img src=\"./MNIST3.png\" align=\"left\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ff944",
   "metadata": {},
   "source": [
    "소프트 맥스(Soft Max)의 동작 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5940cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 10  5]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([3, 10, 5]) # 1차원 배열\n",
    "sess = tf.Session()\n",
    "print(sess.run(a))\n",
    "# argmax() 메소드 배열에서 가장 큰 값을 찾아서 인덱스를 리턴한다.\n",
    "# a 배열에서 10이 가장 크기 때문에 결과는 10의 인덱스인 1이 출력된다.\n",
    "print(sess.run(tf.argmax(a)))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2134548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 10  5]\n",
      " [ 4  5  6]\n",
      " [ 0  8  7]]\n",
      "[1 0 2]\n",
      "[1 0 2]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[3, 10, 5], [4, 5, 6], [0, 8, 7]]) # 2차원 배열\n",
    "sess = tf.Session()\n",
    "print(sess.run(a))\n",
    "# argmax() 메소드를 2차원에서 사용할 경우 적절한 2번째 인수를 지정해야 한다. 생략시 기본값은 0이다.\n",
    "# 2번째 인수를 생략하거나 0을 사용하면 2차원 배열의 각 열에서 최대값의 인덱스를 리턴하고 2번째 인수에 1을 사용하면\n",
    "# 배열의 각 행에서 최대값의 인덱스를 리턴한다.\n",
    "# a 배열에서 열 단위로 최대값을 계산하면 0번째 열의 최대값인 4의 인덱스 1, 1번째 열의 최대값인 10의 인덱스 0,\n",
    "# 2번째 열의 최대값인 7의 인덱스 2가 [1 0 2]와 같이 리턴된다.\n",
    "print(sess.run(tf.argmax(a))) # [1 0 2]\n",
    "print(sess.run(tf.argmax(a, 0))) # [1 0 2]\n",
    "# a 배열에서 행 단위로 최대값을 계산하면 0번째 행의 최대값인 10의 인덱스 1, 1번째 행의 최대값인 6의 인덱스 2,\n",
    "# 2번째 행의 최대값인 8의 인덱스 1이 [1 2 1]와 같이 리턴된다.\n",
    "print(sess.run(tf.argmax(a, 1))) # [1 2 1]\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82418ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder를 만들 때 2차원 이상의 데이터를 기억하는 경우 shape 속성을 이용해서 데이터의 차원을 지정해야 한다.\n",
    "# x, y placeholder의 shape 속성의 첫 번째 값을 None으로 지정한 이유는 데이터 개수의 제약없이 입력받기 위해서이고\n",
    "# 두 번째 속성값을 784와 10으로 지정한 이유는 MNIST 손글씨 이미지의 크기가 28 * 28 = 784 픽셀이고 학습 데이터의\n",
    "# 레이블이 0에서 9까지 10개의 값이기 때문이다.\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 784]) # 학습 데이터(x_train)를 기억할 placeholder 선언한다.\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10]) # 학습 데이터의 레이블(y_train)을 기억할 placeholder 선언한다.\n",
    "keep_prob = tf.placeholder(dtype=tf.float32) # 드롭 아웃 값을 기억할 placeholder 선언한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605fcb3",
   "metadata": {},
   "source": [
    "다층 퍼셉트론을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "157a25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 히든 레이어 부분을 함수로 구현한다.\n",
    "def mlp(x):\n",
    "    # 히든 레이어1\n",
    "    # 784개의 입력을 받는 256개의 뉴런을 사용하기 위해서 가중치를 [입력 데이터 개수, 뉴런의 개수] 만큼의 난수를 만든다.\n",
    "    w1 = tf.Variable(tf.random_uniform([784, 256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1) # 히든 레이어1의 출력\n",
    "    # 히든 레이어2\n",
    "    # 256개의 입력을 받는 128개의 뉴런을 사용하기 위해서 가중치를 [입력 데이터 개수, 뉴런의 개수] 만큼의 난수를 만든다.\n",
    "    w2 = tf.Variable(tf.random_uniform([256, 128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2) # 히든 레이어2의 출력\n",
    "    # 히든 레이어2의 출력에 dropout() 메소드로 드롭 아웃을 적용한다.\n",
    "    # 예전에는 rate=keep_prob 형태로 사용했었는데 버전이 올라가면서 rate=1-keep_prob와 같은 형태로 사용한다.\n",
    "    h2_drop = tf.nn.dropout(h2, rate=1-keep_prob)\n",
    "    # 히든 레이어3\n",
    "    # 128개의 입력을 받는 10개의 뉴런을 사용하기 위해서 가중치를 [입력 데이터 개수, 뉴런의 개수] 만큼의 난수를 만든다.\n",
    "    w3 = tf.Variable(tf.random_uniform([128, 10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    # 다층 퍼셉트론의 출력값을 logit(logistic + probit)로 정의한다. probit는 확률을 재는 단위를 의미한다.\n",
    "    logit = tf.nn.relu(tf.matmul(h2_drop, w3) + b3) # 히든 레이어3의 출력\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63605bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = mlp(x)\n",
    "# logit와 레이블의 크로스 엔트로피를 손실 함수로 사용한다.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y)\n",
    ")\n",
    "\n",
    "# Adam 옵티마이저를 사용해 모델을 최적화 한다.\n",
    "# 모델 최적화 과정은 모델의 예측값과 레이블의 차이를 줄여나가는 과정을 의미한다.\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd44162",
   "metadata": {},
   "source": [
    "조기 종료는 과대 적합을 피하고 충분한 학습을 하기 위해서 학습 중간마다 검증 데이터(x_val)에 대한 정확도를 측정하면 학습 데이터에 대한 정확도는 계속 증가하는 반면에 검증 데이터에 대한 검증 정확도가 점차 떨어질 경우 학습을 중지하는 것을 말한다.  \n",
    "매 epoch 마다 검증 데이터로 검증 정확도를 측정해서 검증 정확도가 5번 연속으로 검증 정확도의 최대값보다 높지 않을 경우 조기 종료를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a0a47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() # tensorflow에서 학습 모델의 저장 및 로드에 사용할 객체를 선언한다.\n",
    "epoch_cnt = 300 # 조기 종료가 일어나지 않을 경우 최대 학습 횟수를 설정한다.\n",
    "batch_size = 1000 # 1번에 읽어서 처리할 학습 데이터 개수를 설정한다.\n",
    "iteration = len(x_train) // batch_size # batch_size에 따른 1 epoch 당 학습 횟수를 설정한다.\n",
    "earlystop = 5 # 검증 정확도가 검증 정확도의 최대값 보다 5번 연속으로 높지 않을 경우 조기 종료하도록 설정한다.\n",
    "earlystop_cnt = 0 # 검증 정확도가 검증 정확도의 최대값 보다 연속으로 높지 않은 검증 횟수를 세는 변수를 선언한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14702efb",
   "metadata": {},
   "source": [
    "학습을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "334e6d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1, 학습 정확도: 0.21354, 검증 정확도: 0.21720\n",
      "epoch:   2, 학습 정확도: 0.52822, 검증 정확도: 0.54310\n",
      "epoch:   3, 학습 정확도: 0.60704, 검증 정확도: 0.63260\n",
      "epoch:   4, 학습 정확도: 0.67774, 검증 정확도: 0.69820\n",
      "epoch:   5, 학습 정확도: 0.71844, 검증 정확도: 0.73790\n",
      "epoch:   6, 학습 정확도: 0.74798, 검증 정확도: 0.76840\n",
      "epoch:   7, 학습 정확도: 0.77262, 검증 정확도: 0.79060\n",
      "epoch:   8, 학습 정확도: 0.79344, 검증 정확도: 0.80770\n",
      "epoch:   9, 학습 정확도: 0.80988, 검증 정확도: 0.82120\n",
      "epoch:  10, 학습 정확도: 0.82152, 검증 정확도: 0.83330\n",
      "epoch:  11, 학습 정확도: 0.83512, 검증 정확도: 0.84500\n",
      "epoch:  12, 학습 정확도: 0.84356, 검증 정확도: 0.85190\n",
      "epoch:  13, 학습 정확도: 0.85210, 검증 정확도: 0.85990\n",
      "epoch:  14, 학습 정확도: 0.85980, 검증 정확도: 0.86700\n",
      "epoch:  15, 학습 정확도: 0.86592, 검증 정확도: 0.87330\n",
      "epoch:  16, 학습 정확도: 0.87228, 검증 정확도: 0.87840\n",
      "epoch:  17, 학습 정확도: 0.87778, 검증 정확도: 0.88310\n",
      "epoch:  18, 학습 정확도: 0.88322, 검증 정확도: 0.88750\n",
      "epoch:  19, 학습 정확도: 0.88680, 검증 정확도: 0.89000\n",
      "epoch:  20, 학습 정확도: 0.89110, 검증 정확도: 0.89410\n",
      "epoch:  21, 학습 정확도: 0.89450, 검증 정확도: 0.89580\n",
      "epoch:  22, 학습 정확도: 0.89836, 검증 정확도: 0.90000\n",
      "epoch:  23, 학습 정확도: 0.90132, 검증 정확도: 0.90190\n",
      "epoch:  24, 학습 정확도: 0.90420, 검증 정확도: 0.90330\n",
      "epoch:  25, 학습 정확도: 0.90844, 검증 정확도: 0.90700\n",
      "epoch:  26, 학습 정확도: 0.91042, 검증 정확도: 0.90860\n",
      "epoch:  27, 학습 정확도: 0.91342, 검증 정확도: 0.90940\n",
      "epoch:  28, 학습 정확도: 0.91664, 검증 정확도: 0.91250\n",
      "epoch:  29, 학습 정확도: 0.91896, 검증 정확도: 0.91310\n",
      "epoch:  30, 학습 정확도: 0.92072, 검증 정확도: 0.91530\n",
      "epoch:  31, 학습 정확도: 0.92322, 검증 정확도: 0.91660\n",
      "epoch:  32, 학습 정확도: 0.92590, 검증 정확도: 0.91810\n",
      "epoch:  33, 학습 정확도: 0.92772, 검증 정확도: 0.91980\n",
      "epoch:  34, 학습 정확도: 0.92906, 검증 정확도: 0.92050\n",
      "epoch:  35, 학습 정확도: 0.93110, 검증 정확도: 0.92130\n",
      "epoch:  36, 학습 정확도: 0.93278, 검증 정확도: 0.92420\n",
      "epoch:  37, 학습 정확도: 0.93456, 검증 정확도: 0.92390\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  38, 학습 정확도: 0.93598, 검증 정확도: 0.92450\n",
      "epoch:  39, 학습 정확도: 0.93758, 검증 정확도: 0.92560\n",
      "epoch:  40, 학습 정확도: 0.93852, 검증 정확도: 0.92640\n",
      "epoch:  41, 학습 정확도: 0.94044, 검증 정확도: 0.92670\n",
      "epoch:  42, 학습 정확도: 0.94174, 검증 정확도: 0.92870\n",
      "epoch:  43, 학습 정확도: 0.94296, 검증 정확도: 0.92860\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  44, 학습 정확도: 0.94368, 검증 정확도: 0.92810\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch:  45, 학습 정확도: 0.94504, 검증 정확도: 0.92980\n",
      "epoch:  46, 학습 정확도: 0.94626, 검증 정확도: 0.93030\n",
      "epoch:  47, 학습 정확도: 0.94744, 검증 정확도: 0.93020\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  48, 학습 정확도: 0.94816, 검증 정확도: 0.93100\n",
      "epoch:  49, 학습 정확도: 0.94924, 검증 정확도: 0.93370\n",
      "epoch:  50, 학습 정확도: 0.94976, 검증 정확도: 0.93260\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  51, 학습 정확도: 0.95122, 검증 정확도: 0.93290\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch:  52, 학습 정확도: 0.95226, 검증 정확도: 0.93490\n",
      "epoch:  53, 학습 정확도: 0.95266, 검증 정확도: 0.93530\n",
      "epoch:  54, 학습 정확도: 0.95388, 검증 정확도: 0.93560\n",
      "epoch:  55, 학습 정확도: 0.95456, 검증 정확도: 0.93540\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  56, 학습 정확도: 0.95542, 검증 정확도: 0.93750\n",
      "epoch:  57, 학습 정확도: 0.95606, 검증 정확도: 0.93730\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  58, 학습 정확도: 0.95716, 검증 정확도: 0.93750\n",
      "epoch:  59, 학습 정확도: 0.95754, 검증 정확도: 0.93910\n",
      "epoch:  60, 학습 정확도: 0.95770, 검증 정확도: 0.93800\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  61, 학습 정확도: 0.95896, 검증 정확도: 0.93890\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch:  62, 학습 정확도: 0.95922, 검증 정확도: 0.94030\n",
      "epoch:  63, 학습 정확도: 0.96000, 검증 정확도: 0.94010\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  64, 학습 정확도: 0.96080, 검증 정확도: 0.94080\n",
      "epoch:  65, 학습 정확도: 0.96088, 검증 정확도: 0.94140\n",
      "epoch:  66, 학습 정확도: 0.96150, 검증 정확도: 0.94130\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  67, 학습 정확도: 0.96150, 검증 정확도: 0.94110\n",
      "epoch:  68, 학습 정확도: 0.96260, 검증 정확도: 0.94220\n",
      "epoch:  69, 학습 정확도: 0.96280, 검증 정확도: 0.94150\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  70, 학습 정확도: 0.96342, 검증 정확도: 0.94160\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch:  71, 학습 정확도: 0.96432, 검증 정확도: 0.94020\n",
      "과대 적합 경고 횟수: 3\n",
      "epoch:  72, 학습 정확도: 0.96416, 검증 정확도: 0.94180\n",
      "epoch:  73, 학습 정확도: 0.96504, 검증 정확도: 0.94130\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  74, 학습 정확도: 0.96528, 검증 정확도: 0.94150\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch:  75, 학습 정확도: 0.96610, 검증 정확도: 0.94230\n",
      "epoch:  76, 학습 정확도: 0.96522, 검증 정확도: 0.94190\n",
      "epoch:  77, 학습 정확도: 0.96730, 검증 정확도: 0.94260\n",
      "epoch:  78, 학습 정확도: 0.96740, 검증 정확도: 0.94230\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  79, 학습 정확도: 0.96766, 검증 정확도: 0.94340\n",
      "epoch:  80, 학습 정확도: 0.96850, 검증 정확도: 0.94240\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  81, 학습 정확도: 0.96870, 검증 정확도: 0.94270\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch:  82, 학습 정확도: 0.96912, 검증 정확도: 0.94380\n",
      "epoch:  83, 학습 정확도: 0.96958, 검증 정확도: 0.94370\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  84, 학습 정확도: 0.96924, 검증 정확도: 0.94540\n",
      "epoch:  85, 학습 정확도: 0.97036, 검증 정확도: 0.94550\n",
      "epoch:  86, 학습 정확도: 0.97104, 검증 정확도: 0.94530\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  87, 학습 정확도: 0.97106, 검증 정확도: 0.94590\n",
      "epoch:  88, 학습 정확도: 0.97130, 검증 정확도: 0.94530\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  89, 학습 정확도: 0.97178, 검증 정확도: 0.94510\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch:  90, 학습 정확도: 0.97142, 검증 정확도: 0.94560\n",
      "epoch:  91, 학습 정확도: 0.97326, 검증 정확도: 0.94560\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch:  92, 학습 정확도: 0.97210, 검증 정확도: 0.94600\n",
      "epoch:  93, 학습 정확도: 0.97318, 검증 정확도: 0.94710\n",
      "epoch:  94, 학습 정확도: 0.97292, 검증 정확도: 0.94740\n",
      "epoch:  95, 학습 정확도: 0.97396, 검증 정확도: 0.94780\n",
      "epoch:  96, 학습 정확도: 0.97444, 검증 정확도: 0.94840\n",
      "epoch:  97, 학습 정확도: 0.97660, 검증 정확도: 0.94960\n",
      "epoch:  98, 학습 정확도: 0.97536, 검증 정확도: 0.94800\n",
      "epoch:  99, 학습 정확도: 0.97492, 검증 정확도: 0.94620\n",
      "epoch: 100, 학습 정확도: 0.97542, 검증 정확도: 0.94760\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 101, 학습 정확도: 0.97570, 검증 정확도: 0.94840\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 102, 학습 정확도: 0.97484, 검증 정확도: 0.94750\n",
      "epoch: 103, 학습 정확도: 0.97750, 검증 정확도: 0.94880\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 104, 학습 정확도: 0.97756, 검증 정확도: 0.94940\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 105, 학습 정확도: 0.97754, 검증 정확도: 0.94870\n",
      "epoch: 106, 학습 정확도: 0.97554, 검증 정확도: 0.94650\n",
      "epoch: 107, 학습 정확도: 0.97644, 검증 정확도: 0.94820\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 108, 학습 정확도: 0.97774, 검증 정확도: 0.94910\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 109, 학습 정확도: 0.97766, 검증 정확도: 0.94770\n",
      "epoch: 110, 학습 정확도: 0.97776, 검증 정확도: 0.94950\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 111, 학습 정확도: 0.97734, 검증 정확도: 0.94970\n",
      "epoch: 112, 학습 정확도: 0.97720, 검증 정확도: 0.94920\n",
      "epoch: 113, 학습 정확도: 0.97938, 검증 정확도: 0.94980\n",
      "epoch: 114, 학습 정확도: 0.97936, 검증 정확도: 0.95140\n",
      "epoch: 115, 학습 정확도: 0.97960, 검증 정확도: 0.95190\n",
      "epoch: 116, 학습 정확도: 0.97990, 검증 정확도: 0.94970\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 117, 학습 정확도: 0.97982, 검증 정확도: 0.95040\n",
      "epoch: 118, 학습 정확도: 0.97950, 검증 정확도: 0.95040\n",
      "epoch: 119, 학습 정확도: 0.97754, 검증 정확도: 0.95010\n",
      "epoch: 120, 학습 정확도: 0.98064, 검증 정확도: 0.94970\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 121, 학습 정확도: 0.98110, 검증 정확도: 0.95120\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 122, 학습 정확도: 0.97906, 검증 정확도: 0.94930\n",
      "epoch: 123, 학습 정확도: 0.98018, 검증 정확도: 0.94980\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 124, 학습 정확도: 0.98212, 검증 정확도: 0.95090\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 125, 학습 정확도: 0.97864, 검증 정확도: 0.95080\n",
      "epoch: 126, 학습 정확도: 0.97798, 검증 정확도: 0.94950\n",
      "epoch: 127, 학습 정확도: 0.97882, 검증 정확도: 0.94830\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 128, 학습 정확도: 0.97876, 검증 정확도: 0.94810\n",
      "epoch: 129, 학습 정확도: 0.98022, 검증 정확도: 0.94920\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 130, 학습 정확도: 0.97770, 검증 정확도: 0.94560\n",
      "epoch: 131, 학습 정확도: 0.97908, 검증 정확도: 0.94810\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 132, 학습 정확도: 0.98108, 검증 정확도: 0.94910\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 133, 학습 정확도: 0.98282, 검증 정확도: 0.95130\n",
      "과대 적합 경고 횟수: 3\n",
      "epoch: 134, 학습 정확도: 0.98262, 검증 정확도: 0.95140\n",
      "epoch: 135, 학습 정확도: 0.98550, 검증 정확도: 0.95240\n",
      "epoch: 136, 학습 정확도: 0.98490, 검증 정확도: 0.95350\n",
      "epoch: 137, 학습 정확도: 0.98602, 검증 정확도: 0.95490\n",
      "epoch: 138, 학습 정확도: 0.98568, 검증 정확도: 0.95320\n",
      "epoch: 139, 학습 정확도: 0.98528, 검증 정확도: 0.95220\n",
      "epoch: 140, 학습 정확도: 0.98476, 검증 정확도: 0.95240\n",
      "epoch: 141, 학습 정확도: 0.98658, 검증 정확도: 0.95470\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 142, 학습 정확도: 0.98690, 검증 정확도: 0.95510\n",
      "epoch: 143, 학습 정확도: 0.98702, 검증 정확도: 0.95570\n",
      "epoch: 144, 학습 정확도: 0.98754, 검증 정확도: 0.95520\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 145, 학습 정확도: 0.98828, 검증 정확도: 0.95740\n",
      "epoch: 146, 학습 정확도: 0.98784, 검증 정확도: 0.95680\n",
      "epoch: 147, 학습 정확도: 0.98836, 검증 정확도: 0.95650\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 148, 학습 정확도: 0.98732, 검증 정확도: 0.95530\n",
      "epoch: 149, 학습 정확도: 0.98922, 검증 정확도: 0.95760\n",
      "epoch: 150, 학습 정확도: 0.98744, 검증 정확도: 0.95760\n",
      "epoch: 151, 학습 정확도: 0.98778, 검증 정확도: 0.95710\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 152, 학습 정확도: 0.98648, 검증 정확도: 0.95450\n",
      "epoch: 153, 학습 정확도: 0.98742, 검증 정확도: 0.95820\n",
      "epoch: 154, 학습 정확도: 0.98874, 검증 정확도: 0.95770\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 155, 학습 정확도: 0.98812, 검증 정확도: 0.95760\n",
      "epoch: 156, 학습 정확도: 0.98926, 검증 정확도: 0.95810\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 157, 학습 정확도: 0.98670, 검증 정확도: 0.95770\n",
      "epoch: 158, 학습 정확도: 0.98866, 검증 정확도: 0.95940\n",
      "epoch: 159, 학습 정확도: 0.98758, 검증 정확도: 0.95740\n",
      "epoch: 160, 학습 정확도: 0.98656, 검증 정확도: 0.95720\n",
      "epoch: 161, 학습 정확도: 0.98876, 검증 정확도: 0.95820\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 162, 학습 정확도: 0.98750, 검증 정확도: 0.95660\n",
      "epoch: 163, 학습 정확도: 0.98604, 검증 정확도: 0.95680\n",
      "epoch: 164, 학습 정확도: 0.98756, 검증 정확도: 0.95750\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 165, 학습 정확도: 0.98778, 검증 정확도: 0.95920\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 166, 학습 정확도: 0.98902, 검증 정확도: 0.95890\n",
      "과대 적합 경고 횟수: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 167, 학습 정확도: 0.98808, 검증 정확도: 0.95930\n",
      "epoch: 168, 학습 정확도: 0.99014, 검증 정확도: 0.95990\n",
      "epoch: 169, 학습 정확도: 0.98798, 검증 정확도: 0.95880\n",
      "epoch: 170, 학습 정확도: 0.98986, 검증 정확도: 0.95910\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 171, 학습 정확도: 0.98962, 검증 정확도: 0.95750\n",
      "epoch: 172, 학습 정확도: 0.98846, 검증 정확도: 0.96060\n",
      "epoch: 173, 학습 정확도: 0.98912, 검증 정확도: 0.95960\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 174, 학습 정확도: 0.98930, 검증 정확도: 0.95810\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 175, 학습 정확도: 0.98962, 검증 정확도: 0.95810\n",
      "과대 적합 경고 횟수: 3\n",
      "epoch: 176, 학습 정확도: 0.98804, 검증 정확도: 0.95860\n",
      "epoch: 177, 학습 정확도: 0.98904, 검증 정확도: 0.95780\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 178, 학습 정확도: 0.98912, 검증 정확도: 0.95760\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 179, 학습 정확도: 0.98888, 검증 정확도: 0.95690\n",
      "epoch: 180, 학습 정확도: 0.98870, 검증 정확도: 0.95740\n",
      "epoch: 181, 학습 정확도: 0.99004, 검증 정확도: 0.96190\n",
      "epoch: 182, 학습 정확도: 0.99158, 검증 정확도: 0.96110\n",
      "과대 적합 경고 횟수: 1\n",
      "epoch: 183, 학습 정확도: 0.99104, 검증 정확도: 0.95870\n",
      "과대 적합 경고 횟수: 2\n",
      "epoch: 184, 학습 정확도: 0.99096, 검증 정확도: 0.95890\n",
      "과대 적합 경고 횟수: 3\n",
      "epoch: 185, 학습 정확도: 0.99236, 검증 정확도: 0.95960\n",
      "과대 적합 경고 횟수: 4\n",
      "epoch: 186, 학습 정확도: 0.99228, 검증 정확도: 0.95990\n",
      "과대 적합 경고 횟수: 5\n",
      "epoch: 187, 학습 정확도: 0.99120, 검증 정확도: 0.95980\n",
      "조기 종료 시점: 186 epoch\n",
      "현재 검증 정확도가 최대 검증 정확도 보다 연속으로 5번 작게나와서 조기 종료가 실행됨\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    prev_train_acc = 0.0 # 이전 학습 정확도를 기억할 변수를 선언한다.\n",
    "    max_val_acc = 0.0 # 검증 정확도의 최대값을 기억할 변수를 선언한다.\n",
    "    # 지정한 최대 epoch 만큼 반복하며 학습한다.\n",
    "    # 검증 정확도가 검증 정확도의 최대값보다 5번 연속으로 높지 않을 경우 조기 종료한다.\n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.0 # epoch 당 손실 값을 기억할 변수를 선언한다.\n",
    "        start = 0 # 학습 시작 위치\n",
    "        end = batch_size # 학습 종료 위치\n",
    "        \n",
    "        # 1 epoch 학습시 학습 데이터를 batch_size개 만큼씩 나눠서 학습을 진행한다.\n",
    "        for i in range(iteration):\n",
    "            _, loss_ = sess.run([train, loss], feed_dict={x: x_train[start:end], y: y_train[start:end], keep_prob: 0.9})\n",
    "            # 학습할 데이터의 범위를 batch_size개 만큼 이동시킨다.\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            # 크로스 엔트로피 손실 함수의 학습 손실을 계산한다.\n",
    "            avg_loss += loss_ / iteration\n",
    "        # ===== for i\n",
    "        \n",
    "        # 1 epoch 학습시 모델 검증을 진행한다.\n",
    "        # 소프트 맥스를 적용해서 예측한다.\n",
    "        predict = tf.nn.softmax(logits=logit)\n",
    "        # 정확도를 계산하는 수식을 만든다.\n",
    "        correct_predict = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predict, 'float'))\n",
    "        \n",
    "        # Session().run()와 Tensor.eval()의 차이\n",
    "        # t가 Tensor 객체라면 t.eval()은 sess.run(t)의 속기 표현이다. 단, sess가 현재 세션인 곳에서만 가능하다.\n",
    "        # 학습 정확도를 계산한다.\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train, keep_prob: 1.0})\n",
    "        # 검증 정확도를 계산한다.\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # epoch 당 학습 정확도와 검증 정확도를 출력한다.\n",
    "        print('epoch: {:3d}, 학습 정확도: {:7.5f}, 검증 정확도: {:7.5f}'.format(epoch + 1, cur_train_acc, cur_val_acc))\n",
    "        \n",
    "        # 최대 검증 정확도와 현재 검증 정확도를 비교한다.\n",
    "        if cur_val_acc < max_val_acc:\n",
    "            # 현재 검증 정확도가 최대 검증 정확도 미만이면\n",
    "            # 현재 학습 정확도와 이전 학습 정확도, 현재 학습 정확도와 0.99를 비교한다.\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                # 현재 학습 정확도가 이전 학습 정확도 보다 크거나 현재 학습 정확도가 0.99 보다 크면\n",
    "                if earlystop_cnt == earlystop:\n",
    "                    print('조기 종료 시점: {} epoch'.format(epoch))\n",
    "                    print('현재 검증 정확도가 최대 검증 정확도 보다 연속으로 {}번 작게나와서 조기 종료가 실행됨'.format(earlystop))\n",
    "                    break\n",
    "                else:\n",
    "                    # 5번 연속으로 현재 학습 정확도가 이전 학습 정확도 보다 크지 않다면 이전 학습 정확도가\n",
    "                    # 현재 학습 정확도 보다 크므로 현재 검증 정확도가 최대 검증 정확도 보다 연속으로 크지 않은 횟수를\n",
    "                    # 카운트 하는 변수를 1증가 시킨다.\n",
    "                    earlystop_cnt += 1\n",
    "                    print('과대 적합 경고 횟수: {}'. format(earlystop_cnt))\n",
    "                # ===== if earlystop_cnt == earlystop:\n",
    "            else:\n",
    "                # 현재 학습 정확도가 이전 학습 정확도 이하이고 현재 학습 정확도가 0.99 이하이면\n",
    "                # 이전 학습 정확도가 현재 학습 정확도 이상이므로 현재 검증 정확도가 검증 정확도의 최대값보다\n",
    "                # 연속으로 높지 않은 횟수를 카운트 하는 변수를 0으로 초기화 시킨다.\n",
    "                earlystop_cnt = 0\n",
    "            # ===== if cur_train_acc > prev_train_acc\n",
    "        else:\n",
    "            # 현재 검증 정확도가 최대 검증 정확도 이상이면\n",
    "            # 현재 검증 정확도가 최대 검증 정확도 보다 연속으로 높지 않은 횟수를 기억하는 변수를 0으로\n",
    "            # 초기화시키고 최대 검증 정확도를 현재 검증 정확도로 교체한다.\n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            # 검증 정확도가 가장 높은 모델을 저장한다.\n",
    "            saver.save(sess, './model/model.ckpt')\n",
    "        # ===== if cur_val_acc < max_val_acc:\n",
    "        # 다음 epoch를 위해 현재 학습 정확도를 이전 학습 정확도를 기억하는 변수에 넣어준다.\n",
    "        prev_train_acc = cur_train_acc\n",
    "    # ===== for epoch\n",
    "# ===== with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc2da2",
   "metadata": {},
   "source": [
    "검증 결과가 가장 높았언 모델을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07395776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n",
      "학습 정확도: 0.99004\n",
      "테스트 정확도: 0.95700\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './model/model.ckpt')\n",
    "    predict = tf.nn.softmax(logits=logit)\n",
    "    correct_predict = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, 'float'))\n",
    "    correct_predict = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, 'float'))\n",
    "    print('학습 정확도: {:7.5f}'.format(accuracy.eval({x: x_train, y: y_train, keep_prob: 1.0})))\n",
    "    print('테스트 정확도: {:7.5f}'.format(accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c2128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd3d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
