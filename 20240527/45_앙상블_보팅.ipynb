{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e5f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%config Completer.use_jedi = False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'NanumGothicCoding'\n",
    "plt.rcParams['font.size'] = 10\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e012b58",
   "metadata": {},
   "source": [
    "앙상블(Ensemble)\n",
    "\n",
    "지도 학습은 피쳐 데이터와 레이블 데이터를 이용해서 전체 데이터를 분류하는 학습 방법이다.  \n",
    "지도 학습에 사용되는 데이터는 각 피쳐 데이터 마다 올바르게 할당된 라벨링 데이터였고 이를 기반으로 학습 알고리즘을 생성하는 과정을 거쳤었다.\n",
    "\n",
    "앙상블 학습의 핵심 아이디어는 학습 데이터를 기반으로 분류 모델을 여러개 만들고 서로 비교하는 것이다.  \n",
    "앙상블 학습 과정에서 만든 개별 분류기 모형을 분류기(Classifier)라고 하고 여러 개의 분류기를 결합함으로써 개별적인 분류기보다 성능이 뛰어난 최종 분류기를 만드는 것이 앙상블 학습의 목적이다.\n",
    "\n",
    "보팅(Voting)\n",
    "\n",
    "여러 개의 분류 모델의 결과를 대상으로 투표를 통해서 최종 클래스에 레이블을 결정하는 방법이다.\n",
    "\n",
    "분류기가 10개 있다고 했을 때, 특정 데이터에 대해서 7개의 분류기는 클래스1 이라고 예측하고, 나머지 3개의 분류기는 클래스2 라고 예측했을 때 클래스1이 가장 높은 득표수를 보이므로 최종적으로 클래스1로 예측하는 것이다. 이를 다수결 투표라고 하는데 이와 비슷한 방법으로 다수결이 아닌 절반 이상의 분류기의 표를 얻어야 하는 과반수 투표 방식이 있다.\n",
    "\n",
    "개별 분류기는 지도 학습 방법 중 k-최근접 이웃, 로지스틱 회귀, 나이브 베이즈, 의사결정 트리, 서포트 벡터 머신 등 여러가지 알고리즘을 사용해서 다양한 분류 모델을 만들어 사용할 수 있다.\n",
    "\n",
    "붓꽃 데이터를 사용해 붓꽃 종류를 분류하는 모델을 생성하고 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30adbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "(112, 4) (38, 4) (112,) (38,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "raw_data = datasets.load_iris() # 사이킷런이 제공하는 붓꽃 데이터를 불러온다.\n",
    "\n",
    "# 피쳐, 레이블 데이터 저장\n",
    "xData = raw_data.data # 피쳐 데이터를 저장한다.\n",
    "yData = raw_data.target # 피쳐 데이터에 따른 레이블을 저장한다.\n",
    "print(xData.shape, yData.shape)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(xData, yData, random_state=0)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# 데이터 표준화\n",
    "std_scale = StandardScaler() # 표준화 스케일러 객체를 만든다.\n",
    "x_train = std_scale.fit_transform(x_train) # 학습 데이터를 스케일러로 표준화 하고 적용한다.\n",
    "x_test = std_scale.transform(x_test) # 테스트 데이터를 학습 데이터로 표준화 스케일러에 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6aa2c",
   "metadata": {},
   "source": [
    "모델 생성 후 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3b6771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('svm', SVC(probability=True)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 weights=[1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 로지스틱 회귀 알고리즘을 사용하기 위해 import 한다.\n",
    "from sklearn.svm import SVC # 서포트 벡터 머신 알고리즘을 사용하기 위해 import 한다.\n",
    "from sklearn.naive_bayes import GaussianNB # 가우시안 나이브 베이즈 알고리즘을 사용하기 위해 import 한다.\n",
    "from sklearn.ensemble import VotingClassifier # 앙상블 보팅 알고리즘을 사용하기 위해 import 한다.\n",
    "\n",
    "clf1 = LogisticRegression() # 로지스틱 회귀 알고리즘을 사용하는 개별 분류기를 만든다.\n",
    "clf2 = SVC(probability=True) # 서포트 벡터 머신 알고리즘을 사용하는 개별 분류기를 만든다.\n",
    "clf3 = GaussianNB() # 가우시안 나이브 베이즈 알고리즘을 사용하는 개별 분류기를 만든다.\n",
    "\n",
    "# 위 3개의 개별 분류기를 이용해서 앙상블 보팅 모델을 만든다.\n",
    "# estimators 속성으로 앙상블 보팅 모델이 사용할 개별 분류기를 지정한다.\n",
    "# voting 속성으로 투표 방식을 지정한다. 'hard'는 기본값으로 과반수 투표를 'soft'는 다수결 투표를 한다.\n",
    "# soft 보팅을 사용할 경우 개별 분류기를 구성하는 서포트 벡터 머신 모델이 있다면 probability=True 속성을\n",
    "# 반드시 지정해야 에러가 발생되지 않는다.\n",
    "# weights 속성으로 개별 분류기에 가중치를 지정해서 가중치 투표를 할 수 있다.\n",
    "clf = VotingClassifier(estimators=[('lr', clf1), ('svm', clf2), ('gnb', clf3)], voting='hard', weights=[1, 1, 1])\n",
    "# 표준화된 학습 데이터와 학습 데이터에 따른 레이블 데이터로 앙상블 보팅 모델을 학습시킨다.\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca7c1c",
   "metadata": {},
   "source": [
    "학습된 모델로 테스트 데이터를 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba136d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "# predict() 메소드의 인수로 표준화된 테스트 데이터를 넘겨서 예측한다.\n",
    "predict = clf.predict(x_test)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc47bf6c",
   "metadata": {},
   "source": [
    "학습된 모델을 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceff2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "# 혼동 행렬\n",
    "# confusion_matrix() 메소드의 인수를 테스트 데이터의 실제값, 예측값 순서로 넘겨서 혼동 행렬를 만든다.\n",
    "conf_matrix = confusion_matrix(y_test, predict)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c09b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        13\n",
      "  versicolor       1.00      0.94      0.97        16\n",
      "   virginica       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 분류 리포트\n",
    "# classification_report() 메소드의 인수를 테스트 데이터의 실제값, 예측값 순서로 넘겨서 분류 리포트를 만든다.\n",
    "# target_names 속성으로 분류 리포트에 레이블의 실제값을 출력할 수 있다.\n",
    "class_report = classification_report(y_test, predict, target_names=raw_data.target_names) \n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9dc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8d4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
