로그(Log) 스태시(Stash)

로그스태시는 로그를 저장하는 기능을 실행한다.
로그는 컴퓨터 시스템에서 시스템의 동작을 기록하는 것으로 성능, 오류, 동작 과정 등의 중요한 정보를 담고있다.
로그는 운영 과정에서 문제 해결을 위해 반드시 필요하며 개발 과정에서도 테스트나 디버깅을 위해 필요한 정보이다.

로그는 형태가 표준화되어 있지 않기 때문에 로그 생산자들은 제각각 다양한 방법으로 로그를 생성한다.
로그라는 것은 반정형(완벽하게 일치하지는 않더라도 어느 정도는 형태가 있는) 데이터이며 세상의 모든 것이 로그가 될 수 있으며 로그의 형태를 강제할 방법도 없기 때문에 로그를 수집하는 쪽에서 로그의 형태를 분석하고 시스템이 인식할 수 있도록 정제하는 작업이 필요한데 로그스태시는 로그를 수집해서 가공(정제)하고 전송하는 일련의 과정을 간편하게 구현하기 위한 기능을 제공한다.

로그스태시는 플러그인 기반의 오픈 소스 데이터 처리 파이프라인 도구이다.
다소 복잡하고 처리하기 귀찮은 데이터를 별도의 애플리케이션 작성 없이 비교적 간단한 설정만으로 수행할 수 있다.
데이터를 저장하기 전에 사용자가 원하는 형태로 변경하는 기능을 제공한다.

# ===========================================================================================================

로그스태시 실행

로그스태시는 자바 가상 머신이 반드시 설치되어 있어야 하고 LS_JAVA_HOME 환경 변수를 JAVA_HOME 환경 변수와 똑같이 설정한다.
로그스태시를 다운받아 압축을 해제한(로그시태시가 설치된) 폴더 아래의 bin 폴더의 logstash.bat 파일을 실행하면 된다.
로그스태시를 실행하기 위해서는 반드시 파이프라인 설정이 필요하다.
파이프라인은 별도의 설정을 만들어 기록하거나 config 폴더의 pipelines.yml 파일에 기록한다.

logstash.bat -e "input { stdin {} } output { stdout {} }" --log.level error
-e 옵션을 사용하면 콘솔에서 직접 파이프라인을 설정할 수 있다.
운영체제의 표준 입력(stdin)으로 전달받은 데이터를 표준 출력(stdout)으로 표시한다.
--log.level error의 의미는 error 레벨 미만의 로그를 감추는 설정이다.

로그 레벨
fatal: 시스템 동작을 멈출 정도의 심각한 오류가 발생할 때 나타는 로그
error: 시스템의 동작을 멈추지는 않지만 오류가 발생할 때 나타는 로그
warn: 잠재적인 오류를 포함하는 경고성 로그
info: 진행 상황이나 상태 변경 등의 정보를 나타내는 로그
debug: 개발 과정에서 디버깅을 하기 위한 로그
trace: 시스템의 진행 과정을 추적하기 위한 로그

실행 중인 로그스태시는 crtl + c를 누르면 종료된다.

# ===========================================================================================================

파이프라인

로그스태시의 가장 중요한 부분은 파이프라인이다.
파이프라인은 데이터를 입력받아 실시간으로 변경하고 이를 다시 시스템에 전달하는 역할을 하는 로그스태시의 핵심 기능이다.
파이프라인은 입력(input)과 출력(output)은 필수 구성 요소이고, 필터(filter)는 선택 사항 이다.
source data => logstash pipeline(input => filter => output) => elastic search

파이프라인 형식
input { # 필수
  {
    입력 플러그인(stdin, file, syslog, kafka, jdbc, ...)
  }
}

filter { # 선택
  {
    필터 플러그인(grok, dissect, mutate, date, ...)
  }
}

output { # 필수
  {
    출력 플러그인(stdout, elasticsearch, file, kafka, ...)
  }
}

# ===========================================================================================================

config 폴더에 logstash-test.conf 라는 이름의 설정 파일을 만든다.
파일에 로그가 쌓이면 실시간으로 파일의 변경된 부분을 감지해 읽어들인다.

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
# "#"으로 시작하면 로그스태시 설정 파일의 주석으로 처리된다.
# 입력으로 file 플러그인을 출력으로 stdout 플러그인을 사용한다.
input {
  file {
    # path는 읽어들일 로그 파일의 위치를 지정한다. 이 때, 폴더와 폴더, 폴더와 파일은 "/"로 구분한다.
    path => "C:/k_digital/kdigital/elasticStack/elasticsearch-7.17.21/logs/elasticsearch.log"
    # start_position은 파일을 최초로 발견했을 때 파일을 읽을 위치를 지정한다.
    # end: 기본값, 끝 부터 새로운 라인만 읽어들인다.
    # beginning: 파일의 시작 부터 읽어 들인다.
    start_position => "beginning"
  }
}

output {
  stdout { }
}

# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test.conf --log.level error
-f 옵션은 파일(..\config\logstash-test.conf)을 파이프라인 설정에 사용한다는 의미이다.

# ===========================================================================================================

실습을 위한 간단한 예제 로그 파일(filter-example.log)을 만든다.

# ***********************************************************************************************************
filter-example.log 파일의 내용
# ***********************************************************************************************************
[2024-06-18 11:47:51] [ID1] 192.168.0.35 9500 [INFO] - connected.
[2024-06-18 11:49:05]   [ID2] 218.35.25.165 1004 [warn] - busy server
# ***********************************************************************************************************

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    # sincedb 데이터베이스 파일은 로그 파일을 어디까지 읽었나 기록하는 파일이다.
    # sincedb_path를 nul로 지정하면 sincedb 데이터베이스 파일을 만들지 않는다.
    # 이전에 파일을 읽었던 기록이 남지 않아서 로그스태시를 실행할 때 마다 매번 파일을 처음부터 다시 읽는다.
    sincedb_path => "nul"
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test01.conf --log.level error

# ===========================================================================================================

필터

입력 플러그인이 받은 데이터를 의미있는 데이터로 구조화하는 역할을 한다.
필수 구성 요소가 아니어서 필터 없이 파이프라인을 구성할 수 있지만 필터가 없는 파이프라인은 기능을 온전히 발휘하지 못한다.

mutate 플러그인
mutate 플러그인은 필드를 변형하는 다양한 옵션을 제공한다.

mutate 플러그인 옵션
mutate 플러그인 많은 옵션이 있어서 순서가 중요하다. 아래에 적어놓은 순서로 옵션이 적용된다.
coerce: null인 필드에 기본값을 넣어준다.
rename: 필드 이름을 바꾼다.
update: 필드 값을 수정한다.
replace: 필드 값을 변경한다.
gsub: 정규식 패턴에 매칭되는 필드 값을 변경한다.
uppercase: 필드 값을 대문자로 변경한다.
capitalize: 필드 값을 첫 문자는 대문자로 나머지는 소문자로 변경한다.
lowercase: 필드 값을 소문자로 변경한다.
strip: 필드 값의 불필요한 공백을 제거한다.
split: 구분자를 기준으로 문자열을 나눠서 배열로 만든다.
join: 구분 문자로 연결해 하나의 문자열로 합친다.
merge: 특정 필드를 다른 필드에 포함시킨다.

# ===========================================================================================================

문자열 자르기
데이터나 로그는 대부분 길이가 길기 때문에 우리가 원하는 형태로 분리해야 한다.
mutate 플러그인에서 split 옵션을 이용한 문자열 자르기

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

# 필터
filter {
  # mutate 플러그인
  mutate {
    # mutate 플러그인 split을 이용해서 message 필드에 저장된 값을 공백(" ")을 기준으로 분리한다.
    split => {
      "message" => " "
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test01.conf --log.level error
필드에 split 옵션을 실행하면 구분자로 구분된 문자열이 필드 이름을 배열 이름으로 아래와 같이 배열로 리턴된다.
"message" => [
[0] "[2024-06-18"
[1] "11:47:51]"
...
[7] "connected.
]
배열의 인덱스에 접근하려면 배열명[인덱스] 형태로 접근하는 것과 같이 필드명[인덱스] 형태로 접근하면 된다.

# ===========================================================================================================

필터에 사용되는 공통 플러그인 옵션
add_field: 새 필드를 추가한다.
remove_field: 기존 필드를 삭제한다.
add_tag: 성공한 이벤트에 태그를 추가한다.
remove_tag: 성공한 이벤트의 태그를 제거한다.
enable_metric: 메트릭 로깅을 활성화하거나 비활성화 한다. 기본적으로 활성화되어 있으며, 로그스태시 모니터링에서 필드의 성능을 분석한다.
id: 플러그인의 아이디를 설정한다. 모니터링 시 특정 플러그인을 쉽게 찾을 수 있다.

필터에 사용되는 공통 플러그인 옵션으로 필드 추가 및 삭제

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  mutate {
    split => {
      "message" => " "
    }
    # 필터 공통 플러그인 옵션 add_field을 이용해서 새 필드를 만들어 추가한다.
    # 공백을 경계로 구분된 message 배열에서 2번째 인덱스 값을 얻어와서 id 필드를 추가한다.
    # 배열의 인덱스 값을 얻어오려면 "%{ ~ }" 내부에 "필드명[인덱스]"를 써줘야 한다.
    # 만약에 [message][2]만 써주면 [message][2] 자체가 문자열로 저장된다.
    add_field => {
      "id" => "%{[message][2]}"
    }
    # 필터 공통 플러그인 옵션 remove_field을 이용해서 기존 필드를 제거한다.
    # "=>" 뒤에 "=>"가 사용되지 않을 경우 {}를 사용하지 않는다.
    remove_field => "message"
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test01.conf --log.level error

# ===========================================================================================================

dissect 플러그인으로 문자열 자르기
dissect 플러그인은 mapping 옵션에 구분자 패턴을 지정해서 문자열을 필드로 저장한다.

# ***********************************************************************************************************
logstash-test02.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  # dissect 플러그인
  dissect {
    mapping => {
      # dissect 플러그인은 mapping을 이용해서 message 필드에 저장된 값을 분리해서 필드로 저장한다.
      # %{필드명}와 같이 작성하면 "%{"와 "}" 사이의 필드명으로 새 필드가 만들어진다.
      # "%{"와 "}" 외부의 문자들은 모두 구분자가 된다.
      # [%{timestamp}] => 1번째 대괄호 사이의 문자열을 읽어서 timestamp 필드를 만들어 저장한다.
      # "[%{timestamp}] [%{id}]"와 같이 패턴을 지정하면 1번째 대괄호와 2번째 대괄호 사이에 공백이 1칸인 로그와 3칸인 로그가
      # 존재하기 때문에 "_dissectfailure" 에러가 발생된다.
      # [2024-06-18 11:47:51] [ID1] 192.168.0.35 9500 [INFO] - connected.
      # [2024-06-18 11:49:05]   [ID2] 218.35.25.165 1004 [warn] - busy server
      # 이런 문제를 해결하려면 연속해서 나타나는 공백을 "%{?->}"를 사용해서 무시하면 된다.
      # [%{id}] => 2번째 대괄호 사이의 문자열을 읽어서 id 필드를 만들어 저장한다.
      # %{ip} => id 다음의 공백과 공백 사이의 문자열을 읽어서 ip 필드를 만들어 저장한다.
      # %{port} => ip 다음의 공백과 공백 사이의 문자열을 port 읽어서 필드를 만들어 저장한다.
      # [%{level}] => 3번째 대괄호 사이의 문자열을 읽어서 level 필드를 만들어 저장한다.
      # %{message} => 마지막 문자열을 읽어서 message 필드를 만들어 저장한다.
      "message" => "[%{timestamp}]%{?->}[%{id}] %{ip} %{port} [%{level}] - %{message}"
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test02.conf --log.level error

# ===========================================================================================================

# ***********************************************************************************************************
logstash-test03.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  dissect {
    mapping => {
      # "%{+필드명}"과 같이 필드명 앞에 "+"를 붙이면 "}" 뒤의 문자열에 "%{+필드명}"에 매핑된 내용을 연결해서 "+"뒤에 지정한 필드에 붙인다.
      # %{ip} %{port} => ip라는 필드와 port라는 필드가 각각 별도로 생성된다.
      # %{ip} %{+ip} => ip라는 필드의 값 뒤에 port 번호를 붙인다.
      # "%{?필드명}"과 같이 필드명 앞에 "?"를 붙이거나 "%{}"만 입력하면 매칭되는 데이터는 버린다.
      "message" => "[%{timestamp}]%{?->}[%{id}] %{ip} %{+ip} [%{?level}] - %{}"
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test03.conf --log.level error

# ===========================================================================================================

grok 플러그인으로 문자열 자르기
grok 플러그인은 match 옵션에 정규식 패턴을 지정해서 문자열을 필드로 저장한다.

# ***********************************************************************************************************
logstash-test04.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  # grok 플러그인
  grok {
    match => {
      # grok 플러그인은 match를 이용해서 message 필드에 저장된 값을 분리(파싱)해서 필드로 저장한다.
      # grok 플러그인은 자주 사용하는 정규식을 패턴화해뒀으며 이를 이용해 "%{패턴:필드명}" 형태로 데이터를 특정 필드로 파싱할 수 있다.
      # grok 플러그인은 자주 사용하는 패턴화된 정규식은 아래 주소를 참조한다.
      # https://github.com/logstash-plugins/logstash-patterns-core/blob/main/patterns/ecs-v1/grok-patterns
      # NUMBER: 십신수를 인식하는 패턴
      # SPACE: 하나 이상의 공백을 인식하는 패턴
      # URI: 웹 주소를 인식하는 패턴
      # IP: IP 주소를 인식하는 패턴
      # LOGLEVEL: 로그 레벨을 인식하는 패턴
      # TIMESTAMP_ISO8601: ISO8601 표준의 날짜(2024-06-19T12:23:27+09:00 형태)를 인식하는 패턴
      # DATA: 데이터를 인식하는 패턴, 직전 패턴부터 다음 패턴 사이를 모두 인식한다.
      # GREEDYDATA: DATA 패턴과 동일하나 정규식 패턴의 마지막에 사용하면 끝까지 모두 데이터로 인식하는 패턴
      # 자주 사용하는 패턴은 이미 만들어져 있기 때문에 가져다 사용하면 되고, 원하는 패턴이 없는 경우 패턴을 직접 만들어 사용한다.

      # "[", "]", "-", "." 같이 구분자로 사용되는(저장할 필요 없는) 기호는 역슬래시(\)를 붙여서 사용한다.
      # \[%{TIMESTAMP_ISO8601:timestamp}\] => 1번째 대괄호 사이의 문자열을 읽어서 timestamp 필드를 만들어 저장한다.
      # "\[%{TIMESTAMP_ISO8601:timestamp}\] \[%{DATA:id}\]"와 같이 패턴을 지정하면 1번째 대괄호와 2번째 대괄호 사이에 
      # 공백이 1칸인 로그와 3칸인 로그가 존재하기 때문에 "_grokparsefailure" 에러가 발생된다.
      # [2024-06-18 11:47:51] [ID1] 192.168.0.35 9500 [INFO] - connected.
      # [2024-06-18 11:49:05]   [ID2] 218.35.25.165 1004 [warn] - busy server
      # 이런 문제를 해결하려면 연속해서 나타나는 공백을 "[ ]*"를 사용해서 무시하면 된다.
      # \[%{DATA:id}\] => 2번째 대괄호 사이의 문자열을 읽어서 id 필드를 만들어 저장한다.
      # %{IP:ip} => id 다음의 공백 1칸 다음의 문자열을 읽어서 ip 필드를 만들어 저장한다.
      # %{NUMBER:port} => ip 다음의 공백 1칸 다음의 문자열을 읽어서 port 필드를 만들어 저장한다.
      # \[%{LOGLEVEL:level}\] => 3번째 대괄호 사이의 문자열을 읽어서 level 필드를 만들어 저장한다.
      # %{GREEDYDATA:msg} => level 다음의 " - " 건너뛰고 끝까지 읽어서 msg 필드를 만들어 저장한다.
      # %{GREEDYDATA:msg}를 사용하지 않고 %{DATA:msg}를 사용하면 "."에 대한 패턴 정의가 없기 때문에 읽어들이지 못해서 msg 필드가
      # 생성되지 않고 "."에 대한 패턴 정의를 위해서 %{DATA:msg} 뒤에 "\."를 붙이면 "."이 없는 데이터는 에러가 발생된다.
      # %{NUMBER:port}의 결과를 엘라스틱서치에 저장하면 문자열 타입으로 매핑된다. 문자열이 아니라 정수 타입으로 엘라스틱서치가 인식하게
      # 하려면 %{NUMBER:port:int}와 같이 ":int"를 추가하면 된다.
      "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] [ ]*\[%{DATA:id}\] %{IP:ip} %{NUMBER:port:int} \[%{LOGLEVEL:level}\] \- %{GREEDYDATA:msg}"
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test04.conf --log.level error

# ===========================================================================================================

실습을 위한 간단한 예제 로그 파일(filter-example2.log)을 만든다.

# ***********************************************************************************************************
filter-example2.log 파일의 내용
# ***********************************************************************************************************
[2024-06-19 11:47:51] [ID1] 192.168.0.35 9500 [INFO] - connected.
[2024/06/19 11:49:05]   [ID2] 218.35.25.165 1004 [warn] - busy server
[2024.06.19 11:49:05]   [ID3] 218.35.25.165 1004 [error] - busy server
# ***********************************************************************************************************

# ***********************************************************************************************************
logstash-test05.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  grok {
    # TIMESTAMP_ISO8601 정규식 패턴은 아래와 같이 정의되어 있고 년, 월, 일의 구분자로 "-"만 허용한다.
    # %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
    # 날짜/시간 데이터는 사용자가 다양한 형태로 만들어 사용하기 때문에 TIMESTAMP_ISO8601 정규식 패턴을 사용하면
    # "2024-06-19"는 정상 처리되지만 "2024/06/19"나 "2024.06.19"는 구분자로 "/", "."을 사용하기 때문에 에러가 발생된다.
    pattern_definitions => {
      # grok 플러그인은 pattern_definitions을 이용해서 사용자 정의 정규식 패턴을 만들어 사용할 수 있다.
      # "사용자 정의 정규식 패턴 이름" => "사용자 정의 정규식 패턴 정의"
      # 년, 월, 일을 "-" 또는 "/"를 사용해 구분하려면 "[-/]", "[/-]" 패턴 모두 사용할 수 있다.
      # 년, 월, 일 구분자로 "."를 추가하려는 경우 "."은 정규식에서 아무 문자 1개와 대응되는 약속된 기능이므로 "."을 문자로
      # 인식되게 하려면 "\\." 처럼 사용해야 한다.
      # [] 내부에 사용하는 "-"은 양쪽에 문자가 나오면 정규식에서 ~부터 ~까지의 의미로 사용되기 때문에 "-"을 문자로 인식되게
      # 하려면 "\-" 처럼 사용해야 한다.
      "USER_TIMESTAMP" => "%{YEAR}[-/\\.]%{MONTHNUM}[/\-\\.]%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?"
    }
    match => {
      "message" => "\[%{USER_TIMESTAMP:timestamp}\] [ ]*\[%{DATA:id}\] %{IP:ip} %{NUMBER:port:int} \[%{LOGLEVEL:level}\] \- %{GREEDYDATA:msg}"
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test05.conf --log.level error

# ===========================================================================================================

대소문자 변경하기
mutate 플러그인에서 uppercase, lowercase, capitalize 옵션을 이용한 대소문자 변경하기

# ***********************************************************************************************************
logstash-test06.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

# 필터
filter {
  dissect {
    mapping => {
      "message" => "[%{?timestamp}]%{?->}[%{id}] %{?ip} %{?port} [%{level}] - %{msg}"
    }
  }
  mutate {
    # mutate 플러그인 uppercase를 이용해서 level 필드에 저장된 값을 모두 대문자로 변경한다.
    # mutate 플러그인 lowercase를 이용해서 level 필드에 저장된 값을 모두 소문자로 변경한다.
    # mutate 플러그인 capitalize를 이용해서 level 필드에 저장된 값을 첫 문자는 대문자로 나머지는 소문자로 변경한다.
    # uppercase => ["level"]
    # lowercase => "id"
    # capitalize => "msg"
    # 2개 이상의 필드에 대문자, 소문자 변경을 적용하려면 []로 묶어서 필드를 나열하면 된다.
    uppercase => ["level", "id", "msg"]
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test06.conf --log.level error

# ===========================================================================================================

날짜/시간 서식 변경하기
date 플러그인에서 날짜/시간 서식 변경하기
엘라스틱서치의 경우 날짜/시간을 ISO8601 포맷을 사용하기 때문에 date 플러그인으로 다양한 날짜/시간 서식을
엘라스틱서치가 사용하는 날짜/시간 서식으로 변경한다.

# ***********************************************************************************************************
logstash-test07.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  dissect {
    mapping => {
      "message" => "[%{timestamp}]%{?->}[%{?id}] %{?ip} %{?port} [%{?level}] - %{?msg}"
      # message 필드에 날짜 시간이 맨 앞에 위치하므로 아래와 같이 실행해도 된다.
      # "message" => "[%{timestamp}]"
    }
  }
  mutate {
    # mutate 플러그인의 strip을 이용해서 timestamp 필드 앞, 뒤의 공백을 제거한다.
    strip => "timestamp"
  }
  # date 필터
  date {
    # date 플러그인은 match를 이용해서 timestamp 필드에 저장된 날짜/시간 서식을 ISO8601로 변경한다.
    # 로그 스태시에서 날짜/시간 서식은 java에서 사용하는 서식과 같다. 자세한 서식은 아래 사이트를 참고한다.
    # https://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html
    # "Y": 년, "M": 월, "d": 일, "H": 시(0 ~ 23), "h": 시(1 ~ 12), "m": 분, "s": 초, "S": 밀리초
    # match에 적어준 서식에 일치하는 날짜/시간 데이터가 없으면 dateparsefailure 에러가 발생된다.
    match => ["timestamp", "YYYY-MM-dd HH:mm:ss", "YYYY/MM/dd HH:mm:ss", "YYYY.MM.dd HH:mm:ss"]
    # date 플러그인은 target을 이용해서 match에서 매칭된 필드가 저장될 새 필드의 이름을 반드시 지정한다.
    target => "new_timestamp"
    # date 플러그인은 timezone을 이용해서 시간대를 설정할 수 있다.
    # timezone 생략시 한국 시간보다 9시간 느린 시간대로 설정되므로 한국 시간으로 하려면 timezone에 "UTC"를 지정한다.
    timezone => "UTC"
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test07.conf --log.level error

# ===========================================================================================================

조건문
필터는 기본적으로 모든 이벤트에 적용된다.
로그스태시는 일반적인 프로그래밍 언어와 동일한 형태로 if, else if, else 조건문을 제공하며, 이를 이용해 이벤트마다
적절한 필터를 적용할 수 있다.

# ***********************************************************************************************************
조건문의 형식
if 조건식 {
  조건이 참일 경우 실행할 내용
} else {
  조건이 거짓일 경우 실행할 내용
}
조건식에 사용하는 필드 이름은 []안에 입력하고 이 때, 필드명 앞뒤에 따옴표는 쓰지 않는다.
# ***********************************************************************************************************

# ***********************************************************************************************************
logstash-test08.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  dissect {
    mapping => {
      "message" => "[%{timestamp}]%{?->}[%{id}] %{ip} %{port} [%{level}] - %{msg}"
    }
  }
  # if 필터
  if [level] == "error" {
    # 로그 레벨이 "error" 도큐먼트를 삭제하는 플러그인 drop으로 삭제한다.
    drop { }
  } else if [level] == "warn" {
    mutate {
      remove_field => ["@timestamp", "@version", "host", "path", "message"]
      add_field => {
        "레벨이" => "error는 아니고 warn인 경우"
      }
    }
  } else {
    mutate {
      remove_field => ["@timestamp", "@version", "host", "path", "message"]
      add_field => {
        "레벨이" => "error는 아니고 warn도 아닌 경우"
      }
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test08.conf --log.level error

# ===========================================================================================================

출력
출력은 로그스태시 파이프라인의 입력과 필터를 거쳐 가공된 데이터를 지정한 대상으로 내보내는 단계이다.
file, elasticsearch 플러그인을 사용해서 엘라스틱서치로 출력한다.
더 많은 출력 플러그인은 아래 사이트를 참조한다.
https://www.elastic.co/guide/en/logstash/7.17/output-plugins.html

# ***********************************************************************************************************
logstash-test09.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

output {
  stdout { }
  # file 플러그인
  file {
    # file 플러그인은 path를 이용해서 출력될 파일의 경로와 이름을 지정한다.
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/output.json"
  }
  # elasticsearch 플러그인
  elasticsearch {
    # elasticsearch 플러그인은 index를 이용해서 출력될 인덱스 이름을 지정한다.
    index => "output"
  }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test09.conf --log.level error

# ===========================================================================================================

코덱(codec)
코덱은 input/filter/output와 달리 독립적으로 동작하지 않고 input과 output 과정에서 사용되는 플러그인이다.
input, output시 적절한 형태로 변환하는 스트림 필터이다.

코덱 플러그인
plain: 기본값, 메시지를 단순 문자열로 읽어들인다.
json: 입력시 json 형태의 메시지를 객체로 읽어들이고 출력시 객체를 json 형태로 변환한다.
rubydebug: 로그스태시 설정을 테스트하거나 파이프라인 오류를 디버깅하기 위한 목적으로 사용된다. 입력시에는 사용하지 않는다.

실습을 위한 간단한 예제 json 파일(sampleJson.json)을 만든다.

# ***********************************************************************************************************
sampleJson.json 파일의 내용
# ***********************************************************************************************************
{"name": "홍길동", "age": "20"}
{"name": "임꺽정", "age": "30"}
{"name": "장길산", "age": "40"}
{"name": "일지매", "age": "50"}
# ***********************************************************************************************************

# ***********************************************************************************************************
logstash-test10.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    # path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/sampleJson.json"
    start_position => "beginning"
    sincedb_path => "nul"
    # codec은 input 작업시 path에 지정된 파일을 내용을 읽어오는 방법을 지정하는 스트림 필터로 생략시 기본값은 "plain" 이다.
    # "plain"은 파일의 내용을 단순 문자열로 읽어서 message 필드를 만들고 저장한다.
    # filter-example2.log 파일이나 sampleJson.json 파일의 내용을 읽어서 message 필드를 만들고 저장한다.
    # codec => "plain"
    # "json"은 파일의 내용을 읽어서 key와 같은 이름의 필드를 만들고 그 필드에 value를 저장한다.
    # 읽어들이는 파일이 json 형태가 아닐 경우 jsonparsefailure 에러가 발생된다.
    # filter-example2.log 파일은 json 형태의 파일이 아니므로 에러가 발생되고 읽은 내용은 message 필드를 만들고 저장한다.
    # sampleJson.json 파일은 json 형태의 파일이므로 message 필드를 만들지 않고 name, age 필드를 만들고 저장한다.
    codec => "json"
    # json 형태의 데이터를 읽어들일 때에는 "json" 코덱을 사용하면 자동으로 필드가 만들어지고 데이터가 저장되므로 편리하다.
    # json 형태가 아닌 데이터를 읽어들일 때는 코덱을 생략하거나 "plain" 코덱을 사용한 후 filter를 이용해서 dissect나
    # grok 플러그인을 사용해서 문자열을 잘라서 필드를 만들고 저장하면 된다.
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test10.conf --log.level error

# ===========================================================================================================

# ***********************************************************************************************************
logstash-test11.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    # path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/sampleJson.json"
    start_position => "beginning"
    sincedb_path => "nul"
    # codec => "plain"
    codec => "json"
  }
}

filter {
  mutate {
    remove_field => ["@timestamp", "@version", "host", "path"]
  }
}

output {
  stdout {
    # codec은 output 작업시 필드에 저장된 데이터를 출력하는 방법을 지정하는 스트림 필터로 생략시 기본값은 "rubydebug" 이다.
    # "line"은 message 필드가 존재하면 message 필드에 저장된 데이터를 라인 형식으로 출력하고 message 필드가 존재하지 않으면
    # "%{message}"가 출력된다.
    # codec => "line"
    # "json"은 모든 필드의 내용이 json 형태로 출력된다.
    # codec => "json"
    # "rubydebug"는 모든 필드의 내용이 들여쓰기가 지원된 json 형태로 출력된다.
    codec => "rubydebug"
  }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test11.conf --log.level error

# ===========================================================================================================

다중 파이프라인

프로젝트를 진행하다 보면 로그 입수 경로가 다양해진다.
A라는 서버와 B라는 서버 두 서버의 로그 형태가 다를 경우 단일 파이프라인을 사용하면 파이프라인이 복잡해지고
요구 사항이 변경될 때마다 점점 더 지저분해지는 문제를 해결하기 위해 다중 파이프라인을 사용한다.
다중 파이프라인은 하나의 로그스태시에서 여러 개의 파이프라인을 독립적으로 실행할 수 있게 한다.

다중 파이프라인을 실행려면 pipelines.yml 파일의 "Example of two pipelines:" 부분을 참고해서 다중 파이프라인을 설정한다.
pipeline.id: 파이프라인의 고유한 아이디를 설정한다.
path.config: 실행할 파이프라인이 저장된 파일의 경로와 이름을 설정한다.
경로 입력시 드라이브 이름(C:)은 입력하지 않고 "\"대신 "/"를 사용해서 폴더와 폴더, 폴더와 파일을 구분한다.

# ***********************************************************************************************************
pipelines.yml 파일의 내용
# ***********************************************************************************************************
...
# Example of two pipelines:
#
# - pipeline.id: test
#   pipeline.workers: 1
#   pipeline.batch.size: 1
#   config.string: "input { generator {} } filter { sleep { time => 1 } } output { stdout { codec => dots } }"
# - pipeline.id: another_test
#   queue.type: persisted
#   path.config: "/tmp/logstash/*.config"

# 다중 파이프라인 설정
- pipeline.id: mypipeline1
  path.config: "/k_digital/kdigital/elasticStack/logstash-7.17.21/config/mypipeline1.conf"
- pipeline.id: mypipeline2
  path.config: "/k_digital/kdigital/elasticStack/logstash-7.17.21/config/mypipeline2.conf"

# Available options:
...
# ***********************************************************************************************************

# ***********************************************************************************************************
mypipeline1.conf 파일의 내용
# ***********************************************************************************************************
# 표준 출력 장치(모니터)로 출력하는 파이프라인
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

# ***********************************************************************************************************
mypipeline2.conf 파일의 내용
# ***********************************************************************************************************
# 엘라스틱서치의 mypipeline2 인덱스에 저장하는 파이프라인
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example2.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

output {
  elasticsearch {
    index => "mypipeline2"
  }
}
# ***********************************************************************************************************

단일 파이프라인을 실행하려면 아래와 같이 실행한다.
logstash.bat -e 실행할_파이프라인 --log.level error
logstash.bat -f 실행할_파이프라인이_저장된_파일명 --log.level error

다중 파이프라인은 복잡하니까 pipelines.yml 파일에 실행할 파이프라인을 설정한다.
로그스태시는 "-e 실행할_파이프라인"이나 "-f 실행할_파이프라인이_저장된_파일명"을 지정하지 않고 아래와 같이 
실행하면 기본적으로 pipelines.yml 파일에 정의된 파이프라인을 실행한다.
logstash.bat --log.level error

# ===========================================================================================================

모니터링
logstash.yml 파일의 X-Pack Settings 부분을 수정한다.

# ***********************************************************************************************************
logstash.yml 파일의 내용
# ***********************************************************************************************************
...
# ------------ X-Pack Settings (not applicable for OSS build)--------------
#
# X-Pack Monitoring
# https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html
#xpack.monitoring.enabled: false # 요기를 아래와 같이 수정한다.
xpack.monitoring.enabled: true # 이렇게
#xpack.monitoring.elasticsearch.username: logstash_system
#xpack.monitoring.elasticsearch.password: password
#xpack.monitoring.elasticsearch.proxy: ["http://proxy:port"]
#xpack.monitoring.elasticsearch.hosts: ["https://es1:9200", "https://es2:9200"] # 요기를 아래와 같이 수정한다.
xpack.monitoring.elasticsearch.hosts: ["http://localhost:9200"] # 이렇게
# an alternative to hosts + username/password settings is to use cloud_id/cloud_auth
...
# ***********************************************************************************************************

logstash.yml 파일의 설정을 변경했으면 로그스태시를 다시 실행해야 변경 사항이 적용된다.

# ===========================================================================================================

키바나에서 모니터링 확인하기

키바나 왼쪽 상단 햄버거 버튼 클릭 => ManageMent => Stack Monitoring 클릭
Stack Monitoring을 한 번도 실행하지 않았으면 "Or, ~~~~~" 링크를 클릭해서 모니터링 UI를 활성화 한다.

# ===========================================================================================================

키바나 Dev Tools에서 로그스태시가 엘라스틱서치에 저장한 데이터 확인하기
GET _cat/indices
GET output/_mapping
GET output/_search
DELETE output

GET mypipeline2/_mapping
GET mypipeline2/_search
