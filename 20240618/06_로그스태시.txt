로그(Log) 스태시(Stash)

로그스태시는 로그를 저장하는 기능을 실행한다.
로그는 컴퓨터 시스템에서 시스템의 동작을 기록하는 것으로 성능, 오류, 동작 과정 등의 중요한 정보를 담고있다.
로그는 운영 과정에서 문제 해결을 위해 반드시 필요하며 개발 과정에서도 테스트나 디버깅을 위해 필요한 정보이다.

로그는 형태가 표준화되어 있지 않기 때문에 로그 생산자들은 제각각 다양한 방법으로 로그를 생성한다.
로그라는 것은 반정형(완벽하게 일치하지는 않더라도 어느 정도는 형태가 있는) 데이터이며 세상의 모든 것이 로그가 될 수 있으며 로그의 형태를 강제할 방법도 없기 때문에 로그를 수집하는 쪽에서 로그의 형태를 분석하고 시스템이 인식할 수 있도록 정제하는 작업이 필요한데 로그스태시는 로그를 수집해서 가공(정제)하고 전송하는 일련의 과정을 간편하게 구현하기 위한 기능을 제공한다.

로그스태시는 플러그인 기반의 오픈 소스 데이터 처리 파이프라인 도구이다.
다소 복잡하고 처리하기 귀찮은 데이터를 별도의 애플리케이션 작성 없이 비교적 간단한 설정만으로 수행할 수 있다.
데이터를 저장하기 전에 사용자가 원하는 형태로 변경하는 기능을 제공한다.

# ===========================================================================================================

로그스태시 실행

로그스태시는 자바 가상 머신이 반드시 설치되어 있어야 하고 LS_JAVA_HOME 환경 변수를 JAVA_HOME 환경 변수와 똑같이 설정한다.
로그스태시를 다운받아 압축을 해제한(로그시태시가 설치된) 폴더 아래의 bin 폴더의 logstash.bat 파일을 실행하면 된다.
로그스태시를 실행하기 위해서는 반드시 파이프라인 설정이 필요하다.
파이프라인은 별도의 설정을 만들어 기록하거나 config 폴더의 pipelines.yml 파일에 기록한다.

logstash.bat -e "input { stdin {} } output { stdout {} }" --log.level error
-e 옵션을 사용하면 콘솔에서 직접 파이프라인을 설정할 수 있다.
운영체제의 표준 입력(stdin)으로 전달받은 데이터를 표준 출력(stdout)으로 표시한다.
--log.level error의 의미는 error 레벨 미만의 로그를 감추는 설정이다.

로그 레벨
fatal: 시스템 동작을 멈출 정도의 심각한 오류가 발생할 때 나타는 로그
error: 시스템의 동작을 멈추지는 않지만 오류가 발생할 때 나타는 로그
warn: 잠재적인 오류를 포함하는 경고성 로그
info: 진행 상황이나 상태 변경 등의 정보를 나타내는 로그
debug: 개발 과정에서 디버깅을 하기 위한 로그
trace: 시스템의 진행 과정을 추적하기 위한 로그

실행 중인 로그스태시는 crtl + c를 누르면 종료된다.

# ===========================================================================================================

파이프라인

로그스태시의 가장 중요한 부분은 파이프라인이다.
파이프라인은 데이터를 입력받아 실시간으로 변경하고 이를 다시 시스템에 전달하는 역할을 하는 로그스태시의 핵심 기능이다.
파이프라인은 입력(input)과 출력(output)은 필수 구성 요소이고, 필터(filter)는 선택 사항 이다.
source data => logstash pipeline(input => filter => output) => elastic search

파이프라인 형식
input { # 필수
  {
    입력 플러그인(stdin, file, syslog, kafka, jdbc, ...)
  }
}

filter { # 선택
  {
    필터 플러그인(grok, dissect, mutate, date, ...)
  }
}

output { # 필수
  {
    출력 플러그인(stdout, elasticsearch, file, kafka, ...)
  }
}

# ===========================================================================================================

config 폴더에 logstash-test.conf 라는 이름의 설정 파일을 만든다.
파일에 로그가 쌓이면 실시간으로 파일의 변경된 부분을 감지해 읽어들인다.

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
# "#"으로 시작하면 로그스태시 설정 파일의 주석으로 처리된다.
# 입력으로 file 플러그인을 출력으로 stdout 플러그인을 사용한다.
input {
  file {
    # path는 읽어들일 로그 파일의 위치를 지정한다. 이 때, 폴더와 폴더, 폴더와 파일은 "/"로 구분한다.
    path => "C:/k_digital/kdigital/elasticStack/elasticsearch-7.17.21/logs/elasticsearch.log"
    # start_position은 파일을 최초로 발견했을 때 파일을 읽을 위치를 지정한다.
    # end: 기본값, 끝 부터 새로운 라인만 읽어들인다.
    # beginning: 파일의 시작 부터 읽어 들인다.
    start_position => "beginning"
  }
}

output {
  stdout { }
}

# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test.conf --log.level error
-f 옵션은 파일(..\config\logstash-test.conf)을 파이프라인 설정에 사용한다는 의미이다.

# ===========================================================================================================

실습을 위한 간단한 예제 로그 파일(filter-example.log)을 만든다.

# ***********************************************************************************************************
filter-example.log 파일의 내용
# ***********************************************************************************************************
[2024-06-18 11:47:51] [ID1] 192.168.0.35 9500 [INFO] - connected.
[2024-06-18 11:49:05]   [ID2] 218.35.25.165 1004 [warn] - busy server
# ***********************************************************************************************************

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    # sincedb 데이터베이스 파일은 로그 파일을 어디까지 읽었나 기록하는 파일이다.
    # sincedb_path를 nul로 지정하면 sincedb 데이터베이스 파일을 만들지 않는다.
    # 이전에 파일을 읽었던 기록이 남지 않아서 로그스태시를 실행할 때 마다 매번 파일을 처음부터 다시 읽는다.
    sincedb_path => "nul"
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test01.conf --log.level error

# ===========================================================================================================

필터

입력 플러그인이 받은 데이터를 의미있는 데이터로 구조화하는 역할을 한다.
필수 구성 요소가 아니어서 필터 없이 파이프라인을 구성할 수 있지만 필터가 없는 파이프라인은 기능을 온전히 발휘하지 못한다.

mutate 플러그인
mutate 플러그인은 필드를 변형하는 다양한 옵션을 제공한다.

mutate 플러그인 옵션
mutate 플러그인 많은 옵션이 있어서 순서가 중요하다. 아래에 적어놓은 순서로 옵션이 적용된다.
coerce: null인 필드에 기본값을 넣어준다.
rename: 필드 이름을 바꾼다.
update: 필드 값을 수정한다.
replace: 필드 값을 변경한다.
gsub: 정규식 패턴에 매칭되는 필드 값을 변경한다.
uppercase: 필드 값을 대문자로 변경한다.
lowercase: 필드 값을 소문자로 변경한다.
strip: 필드 값의 불필요한 공백을 제거한다.
split: 구분자를 기준으로 문자열을 나눠서 배열로 만든다.
join: 구분 문자로 연결해 하나의 문자열로 합친다.
merge: 특정 필드를 다른 필드에 포함시킨다.

# ===========================================================================================================

문자열 자르기
데이터나 로그는 대부분 길이가 길기 때문에 우리가 원하는 형태로 분리해야 한다.
mutate 플러그인에서 split 옵션을 이용한 문자열 자르기

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

# 필터
filter {
  # mutate 플러그인
  mutate {
    # mutate 플러그인 split을 이용해서 message 필드에 저장된 값을 공백(" ")을 기준으로 분리한다.
    split => {
      "message" => " "
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test01.conf --log.level error
필드에 split 옵션을 실행하면 구분자로 구분된 문자열이 필드 이름을 배열 이름으로 아래와 같이 배열로 리턴된다.
"message" => [
[0] "[2024-06-18"
[1] "11:47:51]"
...
[7] "connected.
]
배열의 인덱스에 접근하려면 배열명[인덱스] 형태로 접근하는 것과 같이 필드명[인덱스] 형태로 접근하면 된다.

# ===========================================================================================================

필터에 사용되는 공통 플러그인 옵션
add_field: 새 필드를 추가한다.
remove_field: 기존 필드를 삭제한다.
add_tag: 성공한 이벤트에 태그를 추가한다.
remove_tag: 성공한 이벤트의 태그를 제거한다.
enable_metric: 메트릭 로깅을 활성화하거나 비활성화 한다. 기본적으로 활성화되어 있으며, 로그스태시 모니터링에서 필드의 성능을 분석한다.
id: 플러그인의 아이디를 설정한다. 모니터링 시 특정 플러그인을 쉽게 찾을 수 있다.

필터에 사용되는 공통 플러그인 옵션으로 필드 추가 및 삭제

# ***********************************************************************************************************
logstash-test01.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  mutate {
    split => {
      "message" => " "
    }
    # 필터 공통 플러그인 옵션 add_field을 이용해서 새 필드를 만들어 추가한다.
    # 공백을 경계로 구분된 message 배열에서 2번째 인덱스 값을 얻어와서 id 필드를 추가한다.
    # 배열의 인덱스 값을 얻어오려면 "%{ ~ }" 내부에 "필드명[인덱스]"를 써줘야 한다.
    # 만약에 [message][2]만 써주면 [message][2] 자체가 문자열로 저장된다.
    add_field => {
      "id" => "%{[message][2]}"
    }
    # 필터 공통 플러그인 옵션 remove_field을 이용해서 기존 필드를 제거한다.
    # "=>" 뒤에 "=>"가 사용되지 않을 경우 {}를 사용하지 않는다.
    remove_field => "message"
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test01.conf --log.level error

# ===========================================================================================================

dissect 플러그인으로 문자열 자르기
dissect 플러그인은 mapping 옵션에 구분자 패턴을 지정해서 문자열을 필드로 저장한다.

# ***********************************************************************************************************
logstash-test02.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

# 필터
filter {
  # dissect 플러그인
  dissect {
    mapping => {
      # dissect 플러그인 mapping을 이용해서 message 필드에 저장된 값을 분리해서 필드로 저장한다.
      # %{필드명}와 같이 작성하면 "%{"와 "}" 사이의 필드명으로 새 필드가 만들어진다.
      # "%{"와 "}" 외부의 문자들은 모두 구분자가 된다.
      # [%{timestamp}]% => 1번째 대괄호 사이의 문자열을 timestamp 필드를 만들어 저장한다.
      # "[%{timestamp}]% [%{id}]"와 같이 패턴을 지정하면 1번째 대괄호와 2번째 대괄호 사이에 공백이 1칸 로그와 3칸인 로그가
      # 존재하기 때문에 "_dissectfailure" 에러가 발생된다.
      # [2024-06-18 11:47:51] [ID1] 192.168.0.35 9500 [INFO] - connected.
      # [2024-06-18 11:49:05]   [ID2] 218.35.25.165 1004 [warn] - busy server
      # 이런 문제를 해결하려면 연속해서 나타나는 공백을 "%{?->}"를 사용해서 무시하면 된다.
      # [%{id}] => 2번째 대괄호 사이의 문자열을 id 필드를 만들어 저장한다.
      # %{ip} => id 다음의 공백과 공백 사이의 문자열을 ip 필드를 만들어 저장한다.
      # %{port} => ip 다음의 공백과 공백 사이의 문자열을 port 필드를 만들어 저장한다.
      # [%{level}] => 3번째 대괄호 사이의 문자열을 level 필드를 만들어 저장한다.
      # %{message} => 마지막 문자열을 message 필드를 만들어 저장한다.
      "message" => "[%{timestamp}]%{?->}[%{id}] %{ip} %{port} [%{level}] - %{message}"
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test02.conf --log.level error

# ===========================================================================================================

# ***********************************************************************************************************
logstash-test03.conf 파일의 내용
# ***********************************************************************************************************
input {
  file {
    path => "C:/k_digital/kdigital/elasticStack/logstash-7.17.21/config/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

# 필터
filter {
  # dissect 플러그인
  dissect {
    mapping => {
      # "%{+필드명}"과 같이 필드명 앞에 "+"를 붙이면 "}" 뒤의 문자열에 "%{+필드명}"에 매핑된 내용을 연결해서 "+"뒤에 지정한 필드에 붙인다.
      # %{ip} %{port} => ip라는 필드와 port라는 필드가 각각 별도로 생성된다.
      # %{ip} %{+ip} => ip라는 필드의 값 뒤에 port 번호를 붙인다.
      # "%{?필드명}"과 같이 필드명 앞에 "?"를 붙이거나 "%{}"만 입력하면 매칭되는 데이터는 버린다.
      "message" => "[%{timestamp}]%{?->}[%{id}] %{ip} %{+ip} [%{?level}] - %{}"
    }
  }
}

output {
  stdout { }
}
# ***********************************************************************************************************

logstash.bat -f ..\config\logstash-test03.conf --log.level error

# ===========================================================================================================








